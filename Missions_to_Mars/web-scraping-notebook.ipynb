{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependancies\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import time\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 103.0.5060\n",
      "Get LATEST chromedriver version for 103.0.5060 google-chrome\n",
      "Driver [C:\\Users\\gcwhi\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# set up splinter\n",
    "browser = Browser('chrome',executable_path=ChromeDriverManager().install(),headless=False)\n",
    "# visit Red Planet science site url using splinter\n",
    "url1 = \"https://redplanetscience.com/\"\n",
    "browser.visit(url1)\n",
    "time.sleep(1)\n",
    "# set the variablesand scrape into soup\n",
    "html = browser.html\n",
    "soup = bs(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for title and get one\n",
    "news_title = soup.find('div', class_= 'content_title').get_text()\n",
    "# news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_p = soup.find('div', class_= 'article_teaser_body').get_text()\n",
    "# news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit Space images site url using splinter\n",
    "url2 = \"https://spaceimages-mars.com\"\n",
    "browser.visit(url2)\n",
    "# find full sized image and click\n",
    "full_image = browser.find_by_tag('button')[1]\n",
    "full_image.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables and scrape into soup\n",
    "html = browser.html\n",
    "img_soup = bs(html,\"html.parser\")\n",
    "# find the relative url\n",
    "rel_img_url = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "# rel_img_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the absolute url and save to variable called featured_image_url\n",
    "featured_image_url = f'https://spaceimages-mars.com/{rel_img_url}'\n",
    "# featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html('https://galaxyfacts-mars.com/')[0]\n",
    "#df.head()\n",
    "# rename columns and set index\n",
    "df.columns=['Feature','Mars','Earth']\n",
    "df.set_index('Feature', inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a HTML table string\n",
    "html_table = df.to_html()\n",
    "# print(html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the marshemispheres site to obtain high-resolution images for each hemisphere of Mars.\n",
    "url3 = 'https://marshemispheres.com/'\n",
    "browser.visit(url3)\n",
    "#set variables again for new URL\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "# create an empty list to store a dictionary of image titles and image urls and save to a variable called hemisphere_image_urls\n",
    "hemisphere_image_urls = []\n",
    "# create list to image urls\n",
    "abs_url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all div tags with class description and iterate over them\n",
    "for div in soup.findAll(\"div\", attrs={\"class\":\"description\"}):\n",
    "    #find all hyperlinks\n",
    "    rel_url_list = div.findAll(\"a\")\n",
    "    #iterate over hyperlinks\n",
    "    for link in rel_url_list:\n",
    "        #get the url\n",
    "        href = link['href']\n",
    "        #turn url into full url\n",
    "        abs_url = url3+ href\n",
    "        #append url to list\n",
    "        abs_url_list.append(abs_url)\n",
    "\n",
    "# print url list to check\n",
    "# print(abs_url_list)\n",
    "# print(rel_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in abs_url_list:\n",
    "    url = link\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "        #find all divs with class downloads\n",
    "    for div in soup.findAll(\"div\", attrs={\"class\": \"downloads\"}):\n",
    "        base_url = \"https://marshemispheres.com/\"\n",
    "        #find links with text Original and take href\n",
    "        img_url = base_url + div.find(\"a\", text=\"Sample\")[\"href\"]\n",
    "        #find h2 and get text from h2\n",
    "        h2 = soup.find(\"h2\").get_text()\n",
    "        # remove word Enhanced\n",
    "        title = h2.replace(\"Enhanced\",\"\")\n",
    "        #remove whitespace at end\n",
    "        title = title.rstrip()\n",
    "        #create dictionary to hold results\n",
    "        title_img_dict = {\n",
    "        \"title\": title,\n",
    "        \"img_url\": img_url\n",
    "        }\n",
    "        #append dictionary to list of dictionaries\n",
    "        hemisphere_image_urls.append(title_img_dict)\n",
    "#check list of dictionaries\n",
    "# print(hemisphere_image_urls)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current date/time\n",
    "   \n",
    "last_modified = dt.datetime.now()\n",
    "# print(last_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the splinter browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_mars = {\n",
    "        \"news_title\": news_title,\n",
    "        \"news_p\": news_p,\n",
    "        \"featured_image_url\": featured_image_url,\n",
    "        \"table\": html_table,\n",
    "        \"image_urls\": hemisphere_image_urls,\n",
    "        \"last_modified\": last_modified\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news_title': \"How NASA's Mars Helicopter Will Reach the Red Planet's Surface\",\n",
       " 'news_p': 'The small craft will seek to prove that powered, controlled flight is possible on another planet. But just getting it onto the surface of Mars will take a whole lot of ingenuity.',\n",
       " 'featured_image_url': 'https://spaceimages-mars.com/image/featured/mars2.jpg',\n",
       " 'table': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Mars</th>\\n      <th>Earth</th>\\n    </tr>\\n    <tr>\\n      <th>Feature</th>\\n      <th></th>\\n      <th></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>Mars - Earth Comparison</th>\\n      <td>Mars</td>\\n      <td>Earth</td>\\n    </tr>\\n    <tr>\\n      <th>Diameter:</th>\\n      <td>6,779 km</td>\\n      <td>12,742 km</td>\\n    </tr>\\n    <tr>\\n      <th>Mass:</th>\\n      <td>6.39 × 10^23 kg</td>\\n      <td>5.97 × 10^24 kg</td>\\n    </tr>\\n    <tr>\\n      <th>Moons:</th>\\n      <td>2</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>Distance from Sun:</th>\\n      <td>227,943,824 km</td>\\n      <td>149,598,262 km</td>\\n    </tr>\\n    <tr>\\n      <th>Length of Year:</th>\\n      <td>687 Earth days</td>\\n      <td>365.24 days</td>\\n    </tr>\\n    <tr>\\n      <th>Temperature:</th>\\n      <td>-87 to -5 °C</td>\\n      <td>-88 to 58°C</td>\\n    </tr>\\n  </tbody>\\n</table>',\n",
       " 'image_urls': [{'title': 'Cerberus Hemisphere',\n",
       "   'img_url': 'https://marshemispheres.com/images/full.jpg'},\n",
       "  {'title': 'Schiaparelli Hemisphere',\n",
       "   'img_url': 'https://marshemispheres.com/images/schiaparelli_enhanced-full.jpg'},\n",
       "  {'title': 'Syrtis Major Hemisphere',\n",
       "   'img_url': 'https://marshemispheres.com/images/syrtis_major_enhanced-full.jpg'},\n",
       "  {'title': 'Valles Marineris Hemisphere',\n",
       "   'img_url': 'https://marshemispheres.com/images/valles_marineris_enhanced-full.jpg'}],\n",
       " 'last_modified': datetime.datetime(2022, 6, 30, 5, 22, 41, 836924)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_mars"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
